{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRsFE7QVDRFq"
   },
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftihvSR-DRFt"
   },
   "source": [
    "## Singular Value Decomposition\n",
    "(4.4.2)\n",
    "\n",
    "- 5,000 SMS messages: spam or ham\n",
    "- 16 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYmEmvnPDRFw"
   },
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from nlpia.data.loaders import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69EKnEYCDRFy"
   },
   "outputs": [],
   "source": [
    "# Load the SMS dataset\n",
    "pd.options.display.width = 120  # Just for displaying purposes\n",
    "# the (outdated) nlpia way\n",
    "# sms = get_data('sms-spam')\n",
    "# We can download it directly from their repo\n",
    "url = \"https://raw.githubusercontent.com/totalgood/nlpia/master/src/nlpia/data/sms-spam.csv\"\n",
    "sms = pd.read_csv(url)\n",
    "\n",
    "# Same as before: ! for positive (spam) instances\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms.index = index\n",
    "sms.head(6)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b8TcDZhDRF0"
   },
   "outputs": [],
   "source": [
    "# Compute the tf-idf matrix\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "print(\"number of instances:\\t\", len(tfidf_docs))\n",
    "print(\"size of the vocabulary:\\t\", len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teYzvfwYDRF2"
   },
   "outputs": [],
   "source": [
    "print(tfidf_docs[:10])\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(tfidf_docs[-10:])\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(tfidf_docs[2000:2010])\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(len(tfidf_docs[2000]))\n",
    "print(len(tfidf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC00iCrTDRF3"
   },
   "outputs": [],
   "source": [
    "tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "# Normalization: centers the vectorized documents (BOW vectors) by subtracting the mean\n",
    "# (not the best alternative, but enough in this case)\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean()\n",
    "tfidf_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdPXr9JdDRF4"
   },
   "outputs": [],
   "source": [
    "print(tfidf_docs[:10])\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(tfidf_docs[-10:])\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(tfidf_docs[2000:2010])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yl49EBkVDRF6"
   },
   "outputs": [],
   "source": [
    "sms.spam.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2-fzHEwDRF8"
   },
   "source": [
    "So, in summary, this is what we have:\n",
    "\n",
    "- 4,837 SMS messages\n",
    "- 9,232 different 1-grams\n",
    "- 638 spam messages (13%)\n",
    "- 8:1 ham to spam distribution\n",
    "\n",
    "By consolidating the dimensions (words) into a smaller number of dimensions (topics), the NLP\n",
    "pipeline will become more “general”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf_J_6BUDRF9"
   },
   "source": [
    "## Principal Component Analysis on SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2_E9UlrDRF-"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# This is a crucial parameter. It tells PCA the number of \"topics\" we want\n",
    "pca = PCA(n_components=16)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=index)\n",
    "pca_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uq26dzUDRF_"
   },
   "outputs": [],
   "source": [
    "# We are going to recover the unique index identifiers from our tfidf This is just an index number\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRUAdEpmDRGA"
   },
   "outputs": [],
   "source": [
    "# Sort the vocabulary by term count (we are sorting by value and then displaying the keys)\n",
    "column_nums, terms = zip(*sorted(zip(tfidf.vocabulary_.values(), tfidf.vocabulary_.keys())))\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSxNAjjQDRGA"
   },
   "outputs": [],
   "source": [
    "# This just maps the types to a (unique) index number\n",
    "for c in [\"!\", \"\\\"\", \"between\", \":)\"]:\n",
    "    print(\"'{}':\\t{}\".format(c, tfidf.vocabulary_[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5jeZLHiDRGB"
   },
   "outputs": [],
   "source": [
    "# Sort the vocabulary by term count (we are sorting by value and then displaying the keys)\n",
    "column_nums, terms = zip(*sorted(zip(tfidf.vocabulary_.values(), tfidf.vocabulary_.keys())))\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5B5K9OYDRGB"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(column_nums[i], terms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VM-d7EP_DRGC"
   },
   "outputs": [],
   "source": [
    "# Pandas DataFrame with weights, the words on each topic\n",
    "weights = pd.DataFrame(pca.components_, columns=terms, index=['topic{}'.format(i) for i in range(16)])\n",
    "pd.options.display.max_columns = 8\n",
    "weights.head(5).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vShOoBljDRGC"
   },
   "source": [
    "Checking the topic values for some _typical_ spam words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M__UIwVuDRGD"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 12\n",
    "deals = weights['! ;) :) half off free crazy deal only $ 80 %'.split()].round(3) * 100\n",
    "deals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5mE1qQ7DRGD"
   },
   "source": [
    "Could you identify \"pro-deal\" or \"anti-deal\" topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V0HHVYzDRGD"
   },
   "outputs": [],
   "source": [
    "# Topics 4, 8, and 9 appear to all contain positive “deal” topic sentiment\n",
    "# Topics 0, 3, 5, and 10 appear to be “anti-deal” topics\n",
    "deals.T.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZMqTw7qDRGE"
   },
   "source": [
    "## Truncated SVD for SMS message semantic analysis\n",
    "\n",
    "(Ideal for sparse matrices $\\rightarrow$ better for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq2KgRjJDRGE"
   },
   "outputs": [],
   "source": [
    "# 16 topics\n",
    "# Iterate through the data 100 times (default is 5)\n",
    "svd = TruncatedSVD(n_components= 16, n_iter=100)\n",
    "# Decomposes TF-IDF vectors and transforms them into topic vectors\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_docs.values)\n",
    "svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=columns, index=index)\n",
    "# Same as those produced by PCA\n",
    "svd_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3U7zQKcDRGF"
   },
   "outputs": [],
   "source": [
    "# 2 topics\n",
    "# Iterate through the data 100 times (default is 5)\n",
    "svd = TruncatedSVD(n_iter=100)\n",
    "# Decomposes TF-IDF vectors and transforms them into topic vectors\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_docs.values)\n",
    "svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=[\"topic0\", \"topic1\"], index=index)\n",
    "# Same as those produced by PCA\n",
    "svd_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpRUnVSnDRGF"
   },
   "source": [
    "Computing the cosine similarity **over topic vectors** to see how close (or far) the vectors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYSAi7fJDRGF"
   },
   "outputs": [],
   "source": [
    "# Normalizing each topic vector by its length (L2-norm) allows\n",
    "# to compute the cosine similarity with a dot product\n",
    "svd_topic_vectors = (svd_topic_vectors.T / np.linalg.norm(svd_topic_vectors, axis=1)).T\n",
    "svd_topic_vectors.iloc[:10].dot(svd_topic_vectors.iloc[:10].T).round(1)\n",
    "# Let's analyse columns sms0 and sms2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAMep82MDRGG"
   },
   "source": [
    "### Homework:\n",
    "\n",
    "1. Build a Naive Bayes' classifier using LSA vectors of different dimensions (e.g., 2, 4, 8, 16, 32)\n",
    "2. Build a search engine (semantic search) by integrating our keyword-based retrieval system with this representation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
