{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick overview on preprocessing\n",
    "\n",
    "These materials are mostly borrowed (and extended) from from [Alberto Barron](https://albarron.github.io/teaching/natural-language-processing/) and [Lane et al. (2019)](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "We will discuss some sample code to do the following:\n",
    "\n",
    "1. Remove non-letter characters\n",
    "2. Tokenization\n",
    "3. Remove unwanted parts of speech (PoS Tagging 1)\n",
    "4. Remove unwanted parts of speech based on domain knowledge (PoS Tagging 2)\n",
    "5. Stemming/Lemmatizing\n",
    "6. (A simple) numeric representation: BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import dependencies. In this case, there regex module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Remove non-letter characters\n",
    "This is our first commonly applied dimensionality reduction technique. Very simple, let's look at a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = set('0123456789')\n",
    "punct = set(r\"!@£$%^&*()_`==[]{}\\|'\\\";:/?.><,`~€#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Thomas Jefferson started building Monticello at the age of 26.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas Jefferson started building Monticello at the age of \n"
     ]
    }
   ],
   "source": [
    "clean_txt = \"\"\n",
    "for let in txt:\n",
    "    if let not in numbers:\n",
    "        if let not in punct:\n",
    "            clean_txt += let\n",
    "\n",
    "print(clean_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this seems to work well in this example, it is quite clearly not a reasonable way to do this. Let's have a look at tokenization with regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "Given a character sequence and a defined document, tokenization is the task of chopping it up into meaningful pieces, called *tokens* (perhaps at the same time throwing away certain characters, such as punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'started', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26.']\n"
     ]
    }
   ],
   "source": [
    "# Python comes with in in-built split() method that helps us chop up our document.\n",
    "tokens = txt.split()\n",
    "print(tokens) # Looks reasonable, but not great. Numbers and punctuation is still in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # We can use the regular expression machinery to grab only the parts we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'started', 'building', 'Monticello', 'at', 'the', 'age', 'of']\n"
     ]
    }
   ],
   "source": [
    "# A simple \"tokeniser\", which captures alphabetical characters only.\n",
    "tokens = re.findall('[A-Za-z]+', txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer may be a bit too simplistic ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monticello', 'wasn', 't', 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until']\n"
     ]
    }
   ],
   "source": [
    "text = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987\"\n",
    "tokens = re.findall('[A-Za-z]+', text)\n",
    "print(tokens) # What's the problem here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luckily, there are a lot of open source libraries available with sophisticated tokenizers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "The community has created multiple libraries for pre-processing, which include fucntions to perform tokenisation and many other operations. \n",
    "\n",
    "Two of the most popular ones are\n",
    "\n",
    "* [NLTK](http://www.nltk.org)\n",
    "* [Spacy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is the first time you use them (and this is mostly true if you are using an ephimerous platform, such as colab), you should install it. \n",
    "\n",
    "You can do so with [pip](https://pip.pypa.io/en/stable/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working from the terminal, in local, you might have to do like this (for all dependencies)\n",
    "\n",
    "```\n",
    "$ pip install --user -U spacy\n",
    "```\n",
    "\n",
    "--user tels pip to install it only for you; -U tells to upgrade the package (if it was already installed)\n",
    "\n",
    "**Note:** typically you install all the dependencies on top of the notebook, as it is the first thing you should do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An now we can import and use one of its tokenisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the library\n",
    "import spacy\n",
    "\n",
    "# downloading the model\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(txt)\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# Here is the equivalent process, using NLTK\n",
    "# from nltk.tokenize import TreebankWordTokenizer # import one of the many tokenizers available\n",
    "# tokenizer = TreebankWordTokenizer()             # invoke it \n",
    "# tokens = tokenizer.tokenize(txt)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see the difference between tokenising with split() and with spacy's web tokeniser on a different sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "tokens_split = sentence.split()\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"OUTPUT USING split()\\t\", tokens_split)\n",
    "print(\"OUTPUT USING spacy\\t\", [token.text for token in  doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "### Casefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence  = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Once again, we can use a regular expression to do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
    "         word)[0][0].strip(\"'\") for word in phrase.lower()\n",
    "         .split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'houses' \\t\\t->\", stem('houses'))\n",
    "print(\"'Doctor House's calls' \\t->\", stem(\"Doctor House's calls\"))\n",
    "print(\"'stress' \\t\\t->\", stem(\"stress\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we would need to include many more expressions to deal with all cases and exceptions.\n",
    "\n",
    "Instead, once again we can rely on a library. Let's consider the **Porter stemmer**. NLTK has an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NLTK (and its dependency: numpy)\n",
    "! pip install --user -U nltk\n",
    "! pip install --user -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer # Import the stemmer\n",
    "stemmer = PorterStemmer()                  # invoke the stemmer\n",
    "\n",
    "# Notice:\n",
    "# - this one-liner \"tokenises\", stems, and concatenates, all in one line!\n",
    "# - these operations \"appear\" inverted in the code (let us have a look together) \n",
    "x = ' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])\n",
    "print(x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "This is a more complex process, compared to stemming. Let us use a library.\n",
    "In this particular case we are going to use NLTK's WordNet lemmatiser. If it is the first time you use it (or you are in an ephemeral environment!), you should download it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLTK alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "## Download the Wordnet resources\n",
    "# WordNet core resources\n",
    "nltk.download('wordnet')\n",
    "# Open Multilingual Wordnet resources\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # importing the lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()        # invoking the lemmatiser\n",
    "\n",
    "print(\"'better' alone \\t->\",lemmatizer.lemmatize(\"better\"))\n",
    "print(\"'better' incl. it's POS (adj) \\t->\",lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spacy alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"better\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick overview on representations\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "First, let us see a simple construction, using a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26.', 1),\n",
       " ('Jefferson', 1),\n",
       " ('Monticello', 1),\n",
       " ('Thomas', 1),\n",
       " ('age', 1),\n",
       " ('at', 1),\n",
       " ('began', 1),\n",
       " ('building', 1),\n",
       " ('of', 1),\n",
       " ('the', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26. Thomas\"\"\"\n",
    "\n",
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "     sentence_bow[token] = 1\n",
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option would be using **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might have to install it first\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
       "sent0       1          1      1         1           1   1    1    1   1    1\n",
       "sent1       0          0      0         0           0   0    0    0   0    0\n",
       "sent2       0          0      0         0           0   0    1    0   0    0\n",
       "sent3       0          0      0         0           1   0    0    0   0    0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the corpus\n",
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "# Loading the tokens into a dictionary (notice that we assume that each line is a document)\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
    "         sent.split())\n",
    "\n",
    "# Loading the dictionary contents into a pandas dataframe. \n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "# SEE THE .T, which transposes the matrix for visualisation purposes.\n",
    "\n",
    "\n",
    "df[df.columns[:10]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectors\n",
    "\n",
    "This is our input sentence (and its vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we produce the one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# create the |tokens| x |vocabulary size| matrix of zeros \n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int) \n",
    "print(token_sequence)\n",
    "print(onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(token_sequence):\n",
    "   onehot_vectors[i, vocab.index(word)] = 1  # switch on (1) the right element of the vector\n",
    "\n",
    "print(\"Vocabulary:\\t\", vocab)\n",
    "print(\"Sentence:\\t\", token_sequence)\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us bring **pandas** into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the preprocessing _pipeline_:\n",
    "\n",
    "1. Tokenisation\n",
    "2. Stemmming\n",
    "3. Stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the necessary objects\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tiny test\n",
    "print(tokenizer.tokenize(\"The input text.\"))\n",
    "stemmer.stem(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both tokenisation and stemming\n",
    "text = \"\"\"Perseverance (nicknamed Percy) is a car-sized Mars \n",
    "rover designed to explore the crater Jezero on Mars as part \n",
    "of NASA's Mars 2020 mission.\"\"\"\n",
    "\n",
    "print([stemmer.stem(w) for w in tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first time you use the stopwords, you have to download them!\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words[:100])\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a stopword?\n",
    "\n",
    "According to [the Wikipedia](https://en.wikipedia.org/wiki/Stop_word): \"the words in a stop list (or stoplist or negative dictionary) which are **filtered out** (i.e. stopped) before or after processing of natural language data (text) because they are insignificant.\"\n",
    "\n",
    "For some search engines, these are **some of the most common, short function words,** such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as \"The Who\", \"The The\", or \"Take That\". \n",
    "\n",
    "**Q: Can I create a list of stopwords on the fly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation and stemming, and stopwording\n",
    "text = \"\"\"Perseverance (nicknamed Percy) is a car-sized Mars \n",
    "rover designed to explore the crater Jezero on Mars as part \n",
    "of NASA's Mars 2020 mission.\"\"\"\n",
    "\n",
    "print([stemmer.stem(w) for w in tokenizer.tokenize(text) if w not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "When each of the components is first introduced, the simplest versions of the others are used. Create a pipeline that performs the whole pre-precessing...\n",
    "\n",
    "1. Using NLTK alone\n",
    "2. Using spacy alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of the notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
